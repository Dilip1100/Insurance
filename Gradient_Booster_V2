import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator

# Read the CSV data
df = pd.read_csv('C:/Users/dilip/Downloads/insurance_claims.csv')  # Adjust the file path

# Handle categorical variables (assuming incident_type and insured_sex are categorical)
le = LabelEncoder()
df["incident_type"] = le.fit_transform(df["incident_type"])
df["insured_sex"] = le.fit_transform(df["insured_sex"])

# Convert policy_bind_date to numerical format (assuming it's a date string)
df["policy_bind_date"] = pd.to_datetime(df["policy_bind_date"]).dt.year

# Feature engineering (exploratory)
df['age_squared'] = df['age'] ** 2  # Example: Square of age for non-linearity
df['premium_log'] = np.log(df['policy_annual_premium'] + 1)  # Example: Log transformation for skewed premiums (add 1 to avoid log(0))

# Separate features (X) and target variable (y)
X = df[["age", "policy_annual_premium", "incident_type", "insured_sex", "policy_bind_date", "age_squared", "premium_log"]]
y = df["total_claim_amount"]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (recommended for Gradient Boosting)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define hyperparameter grid for tuning (experiment with different values)
param_grid = {
    'learning_rate': [0.05, 0.1, 0.2],
    'n_estimators': [100, 200, 300],
    'max_depth': [4, 8, 12]
}

# Create a Gradient Boosting Regressor object with specified random state
model = GradientBoostingRegressor(random_state=42)

# Use GridSearchCV to find the best hyperparameters
grid_search = GridSearchCV(model, param_grid, cv=5, scoring='neg_mean_squared_error')  # Use negative MSE for minimization
grid_search.fit(X_train_scaled, y_train)

# Get the best model and its parameters
best_model = grid_search.best_estimator_
best_params = grid_search.best_params_
print("Best Parameters:", best_params)

# Make predictions on the testing set using the best model
y_pred = best_model.predict(X_test_scaled)

# Evaluate model performance (mean squared error and R-squared)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Get the policy bind dates for the test set (assuming "policy_bind_date" is a column)
test_years = df.loc[X_test.index, "policy_bind_date"]  # Access policy_bind_date using indices from original X_test

# Plot predicted vs. actual claim amounts as a horizontal bar chart
x = np.arange(len(test_years))  # Create x-axis positions for bars
width = 0.35  # Width of the bars

fig, ax = plt.subplots()
bars1 = ax.barh(x - width/2, y_test.reset_index(drop=True), width, label='Actual Claim Amount')
bars2 = ax.barh(x + width/2, y_pred, width, label='Predicted Claim Amount')

# Set labels and title
ax.set_xlabel('Claim Amount')
ax.set_ylabel('Claim Year')
ax.set_title('Gradient Boosting Predictions vs. Actual Claims')
ax.set_yticks(x)
ax.set_yticklabels(test_years)
ax.legend()

# Reduce the density of y-axis labels
ax.yaxis.set_major_locator(MaxNLocator(integer=True, nbins=10))  # Adjust nbins to control the number of labels

fig.tight_layout()

# Save the plot to the root directory
plt.savefig('C:/Users/dilip/insurance_predictions_plot.png')
plt.show()

# Forecast claim amounts until year 2020
years_to_forecast = np.arange(2016, 2021)  # Years from 2016 to 2020
num_forecast_years = len(years_to_forecast)

# Example feature values for forecasting (use the same format as your original data)
example_age = 40
example_policy_annual_premium = 1500
example_incident_type = 1
example_insured_sex = 0
example_age_squared = example_age ** 2
example_premium_log = np.log(example_policy_annual_premium + 1)

# Create a dictionary with example values for each feature
forecast_data = {
    'age': [example_age] * num_forecast_years,
    'policy_annual_premium': [example_policy_annual_premium] * num_forecast_years,
    'incident_type': [example_incident_type] * num_forecast_years,
    'insured_sex': [example_insured_sex] * num_forecast_years,
    'policy_bind_date': years_to_forecast,
    'age_squared': [example_age_squared] * num_forecast_years,
    'premium_log': [example_premium_log] * num_forecast_years
}

# Create DataFrame for forecasting
df_predict = pd.DataFrame(forecast_data)

# Scale the features for forecasting
X_predict_scaled = scaler.transform(df_predict)

# Make predictions for the forecasted years
predictions = best_model.predict(X_predict_scaled)

# Prepare prediction output as DataFrame
prediction_df = pd.DataFrame({
    "Year": df_predict["policy_bind_date"],
    "Predicted Claim Amount": predictions
})

# Save predictions to CSV file in the root directory
prediction_df.to_csv('C:/Users/dilip/insurance_predictions_until_2020.csv', index=False)
