import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Read the CSV data
df = pd.read_csv('C:/Users/dilip/Downloads/insurance_claims.csv')

# Handle categorical variables (assuming incident_type and insured_sex are categorical)
le = LabelEncoder()
df["incident_type"] = le.fit_transform(df["incident_type"])
df["insured_sex"] = le.fit_transform(df["insured_sex"])

# Convert policy_bind_date to numerical format (assuming it's a date string)
df["policy_bind_date"] = pd.to_datetime(df["policy_bind_date"]).dt.year  # Extract year

# Feature engineering (exploratory)
# Consider creating new features based on domain knowledge
# Examples (uncomment if desired):
df['age_squared'] = df['age'] ** 2  # Example: Square of age for non-linearity
df['premium_log'] = np.log(df['policy_annual_premium'] + 1)  # Example: Log transformation for skewed premiums (add 1 to avoid log(0))

# Separate features (X) and target variable (y)
X = df[["age", "policy_annual_premium", "incident_type", "insured_sex", "policy_bind_date", "age_squared", "premium_log"]]  # Include new features if created
y = df["total_claim_amount"]

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Check for empty training data before scaling (optional)
print("Training data shape:", X_train.shape)  # Check the shape of training data

# Feature scaling (recommended for Random Forest)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Create and train the Random Forest model
model = RandomForestRegressor(n_estimators=100, random_state=42)  # Adjust n_estimators (number of trees) if needed
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = model.predict(X_test)

# Evaluate model performance (mean squared error and R-squared)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print("Mean Squared Error:", mse)
print("R-squared:", r2)

# Plot predicted vs. actual claim amounts (optional visualization)
plt.scatter(y_test, y_pred)
plt.xlabel("Actual Claim Amount")
plt.ylabel("Predicted Claim Amount")
plt.title("Random Forest Predictions vs. Actual Claims")
plt.show()

# Predict claim amounts for data until year 2020 (assuming your data covers years before 2020)
# Filter data for years 2016-2020 (adjust based on your data range)
df_predict = df[df["policy_bind_date"] >= 2016]  # Assuming data starts before 2016

# Check for empty data before prediction (optional)
print("Data for prediction shape:", df_predict.shape)  # Check the shape of filtered data

if df_predict.shape[0] > 0:
  X_predict = df_predict[["age", "policy_annual_premium", "incident_type", "insured_sex", "policy_bind_date", "age_squared", "premium_log"]]
  X_predict = scaler.transform(X_predict)  # Apply scaling used for training
  predictions = model.predict(X_predict)

  # Prepare prediction output as DataFrame
  prediction_df = pd.DataFrame({"Policy Number": df_predict["policy_number"],  # Assuming a "policy_number" column exists
                                 "Year": df_predict["policy_bind_date"],
                                 "Predicted Claim Amount": predictions})

  # Save predictions to CSV file
  prediction_df.to_csv("insurance_predictions.csv", index=False)  
